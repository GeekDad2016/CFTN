# LSTM Training and Image Generation Explanation

This document explains how the project uses an LSTM to learn the patterns of the VQ-VAE's latent space and generate new images.

## 1. How `train_lstm.py` Works

The goal of the LSTM is to learn the "language" of your images. Since the VQ-VAE has turned every image into a grid of discrete numbers (indices), we can treat an image like a paragraph of text where each "pixel" in the latent grid is a word.

### Data Preparation (`VQVAESeqDataset`)
1.  **Input:** The script loads `encodings.pkl`, which contains the $32 \times 32$ grids of indices generated by `infer_vqvae.py`.
2.  **Flattening:** Each $32 \times 32$ grid is flattened into a single sequence of 1024 numbers.
3.  **Special Tokens:** Two special tokens are added to the vocabulary:
    *   **Start Token:** Tells the LSTM "I am beginning a new image."
    *   **Pad Token:** Used to fill space if a sequence is too short.
4.  **Sliding Window:** The script creates training pairs. For a sequence of indices, it takes a small window (e.g., 32 tokens) as the **Input** and the very next token as the **Target**.

### The Model (`VQVAELSTM`)
*   **Embedding Layer:** Converts the discrete index (0-4095) into a continuous vector that the LSTM can understand.
*   **LSTM Layers:** Processes the sequence of vectors. It maintains an internal "memory" of what it has seen so far in the image.
*   **Linear Layer (FC):** Takes the final output of the LSTM and predicts the probability for every possible next index in the codebook (4096 possibilities).

---\n
## 2. How `generate_images.py` Works

Once the LSTM is trained, it can "write" new images from scratch.

### Auto-regressive Generation
The generation process happens one step at a time:
1.  **Start:** We give the LSTM the **Start Token**.
2.  **Predict:** The LSTM predicts the probabilities for what the first "pixel" (index) of the latent grid should be.
3.  **Sample:** We randomly pick one index based on those probabilities (using `torch.multinomial`).
4.  **Loop:** we feed that new index back into the LSTM as the next input. We repeat this 1024 times until we have a full $32 \times 32$ grid of indices.

### Decoding to Image
1.  **Reshaping:** The 1024 predicted indices are reshaped back into a $32 \times 32$ grid.
2.  **Embedding Lookup:** We use the **VQ-VAE's Codebook** to look up the actual vectors associated with those 1024 indices.
3.  **Decoder Pass:** These vectors are fed into the **VQ-VAE Decoder**.
4.  **Result:** The Decoder upsamples the $32 \times 32$ grid back into a $128 \times 128$ RGB image.

## 3. Summary of the Pipeline
1.  **VQ-VAE:** Learns to compress images into a small grid of "visual words" (the codebook).
2.  **Inference:** Converts your entire dataset into sequences of these visual words.
3.  **LSTM:** Learns which visual words usually follow each other (e.g., if it sees a "blue sky" word, it learns that another "blue sky" word or a "cloud" word is likely to come next).
4.  **Generation:** The LSTM "dreams" a new sequence of visual words, and the VQ-VAE Decoder translates that dream back into a picture.
